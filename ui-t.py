# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'ui-yolov5.ui'
#
# Created by: PyQt5 UI code generator 5.15.9
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

from PyQt5.QtCore import Qt, QPoint, QTimer, QThread, pyqtSignal
from PyQt5.QtGui import QPixmap, QImage
from PyQt5.QtWidgets import QApplication, QMainWindow, QFileDialog, QMenu, QAction
from uiyolov5 import Ui_MainWindow

from models.common import DetectMultiBackend
from utils.dataloaders import letterbox
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors
from utils.torch_utils import select_device
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages
from lane import LANE_DETECTION

import sys
import os
import cv2
import numpy as np
import torch
import time


class detThread(QThread):
    send_img = pyqtSignal(np.ndarray)
    send_raw = pyqtSignal(np.ndarray)
    send_statistic = pyqtSignal(dict)
    # emit：detecting/pause/stop/finished/error msg
    send_msg = pyqtSignal(str)
    send_int = pyqtSignal(int)
    send_fps = pyqtSignal(str)

    def __init__(self, device='', weight='yolov5s.pt', iou=0.45, conf=0.7):
        super(detThread, self).__init__()
        # 权重
        self.weight = weight
        self.cur_weight = weight
        # iou阈值
        self.iou = iou
        # 置信度阈值
        self.conf = conf
        self.device = select_device(device)
        self.half = False  # use FP16 half-precision inference
        self.dnn = False  # use OpenCV DNN for ONNX inference
        self.imgsz = (640, 640)  # inference size (height, width)
        self.line_thickness = 3  # 框线条宽度
        self.max_det = 1000  # 最多检测目标
        self.classes = None  # 是否只保留特定类别: --class 0, or --class 0 2 3
        self.agnostic_nms = False  # 进行nms是否也去除不同类别间的框class-agnostic NMS

        # 是否能够左右变道，默认不能
        self.llane = False
        self.rlane = False
        # 车辆偏离度
        self.devia = 0.0
        # 曲率半径
        self.radio = 0
        # 车道方向，0直行，1左转，2右转
        self.direc = 0
        self.obj = []
        self.dis = []
        # 检测数据路径
        self.source = './data/video'
        self.save = './runs/merge'
        self.end = False
        self.autosave = True
        self.vid_cap = None
        self.out = None
        self.percent_length=1

    def run(self):
        # YOLOv5模型初始化
        model = DetectMultiBackend(self.weight, device=self.device, dnn=self.dnn, fp16=self.half)
        stride, names, pt = model.stride, model.names, model.pt
        imgsz = check_img_size(self.imgsz, s=stride)  # check image size

        # 加载数据集
        dataset = LoadImages(self.source, img_size=imgsz, stride=stride)
        dataset = iter(dataset)

        # Run inference
        bs = 1  # batch_size
        model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup

        count=0     # 记录当前处理对象次数
        while True:
            if self.end:
                # 终止
                self.vid_cap.release()  # yolov5内视频捕获终止
                if self.out is not None:
                    self.out.release()  # 保存视频终止
            if self.cur_weight != self.weight:
                # 更换模型
                model = DetectMultiBackend(self.cur_weight, device=self.device, dnn=self.dnn, fp16=self.half)
                stride, names, pt = model.stride, model.names, model.pt
                imgsz = check_img_size(self.imgsz, s=stride)  # check image size
                bs = 1  # batch_size
                model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
                self.weight = self.cur_weight
            # 计数当前处理帧的个数
            count += 1
            # 获取下一个对象
            path, im, im0s, self.vid_cap, s = next(dataset)
            if self.vid_cap:
                percent = int(count / self.vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)*self.percent_length)
                print(percent,count,self.vid_cap.get(cv2.CAP_PROP_FRAME_COUNT))
            else:
                percent = self.percent_length
            # 图像预处理2
            im = torch.from_numpy(im).to(model.device)  # 图像格式转换
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0归一化
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim

            # 模型推理
            # 图片进行前向推理
            pred = model(im, augment=False, visualize=False)  # augment和visualize都是参数，预测是否采用数据增强、虚拟化特征？
            # nms除去多余的框
            pred = non_max_suppression(pred, self.conf, self.iou, self.classes, self.agnostic_nms, max_det=self.max_det)
            # 每张图片进行处理
            for i, det in enumerate(pred):
                im0 = im0s.copy()
                annotator = Annotator(im0, line_width=self.line_thickness, example=str(names))
                if len(det):
                    # 将坐标信息恢复到原始图像的尺寸
                    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
                    for *xyxy, conf, cls in reversed(det):  # 框位置，置信度和类别id
                        c = int(cls)  # integer class
                        # 保存当前信息
                        '''xyxy_list.append(xyxy)
                        conf_list.append(conf)
                        class_id_list.append(c)'''
                        # 标签内容
                        label = f'{names[c]} {conf:.2f}'
                        annotator.box_label(xyxy, label, color=colors(c, True))
                        '''
                        dist = lane_detect.distance(xyxy)
                        annotator.box_label2(xyxy, dist, color=colors(c, True))
                        '''
                im0 = annotator.result()
                self.send_img.emit(im0)
                # 保存
                if self.autosave:
                    os.makedirs(self.save, exist_ok=True)
                    if self.vid_cap is None:
                        # 图片
                        save_path = os.path.join(self.save,
                                                 time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime()) + '.png')
                        cv2.imwrite(save_path, im0)
                    else:
                        # 视频
                        if count==1:    #第一帧初始化
                            fps = int(self.vid_cap.get(cv2.CAP_PROP_FPS))
                            if fps == 0:
                                fps = 25
                            width, height = im0.shape[1], im0.shape[0]
                            save_path = os.path.join(self.save,
                                                     time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime()) + '.mp4')
                            self.out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))
                        self.out.write(im0)
            if percent==self.percent_length:
                break


class MainWindow(QMainWindow, Ui_MainWindow):
    def __init__(self, parent=None):
        super(MainWindow, self).__init__(parent)
        self.setupUi(self)

        # yolov5 thread
        self.det_thread = detThread()
        self.det_thread.send_img.connect(lambda x: self.show_image(x, self.label_result))

        self.pushButton_4.clicked.connect(self.btn_run)

    @staticmethod
    def show_image(img_src, label):
        try:
            ih, iw, _ = img_src.shape
            w = label.geometry().width()
            h = label.geometry().height()
            # keep original aspect ratio
            if iw / w > ih / h:
                scal = w / iw
                nw = w
                nh = int(scal * ih)
                img_src_ = cv2.resize(img_src, (nw, nh))

            else:
                scal = h / ih
                nw = int(scal * iw)
                nh = h
                img_src_ = cv2.resize(img_src, (nw, nh))

            frame = cv2.cvtColor(img_src_, cv2.COLOR_BGR2RGB)
            img = QImage(frame.data, frame.shape[1], frame.shape[0], frame.shape[2] * frame.shape[1],
                         QImage.Format_RGB888)
            label.setPixmap(QPixmap.fromImage(img))

        except Exception as e:
            print(repr(e))

    def btn_run(self):
        self.det_thread.end = False
        if self.pushButton_4.isChecked():
            # 是否已经按下
            self.pushButton_3.setEnabled(False)
            # self.det_thread.is_continue = True
            if not self.det_thread.isRunning():
                self.det_thread.start()
            source = os.path.basename(self.det_thread.source)
            '''source = 'camera' if source.isnumeric() else source
            self.statistic_msg('Detecting >> model：{}，file：{}'.
                               format(os.path.basename(self.det_thread.weights),
                                      source))'''
        '''else:
            self.det_thread.is_continue = False
            self.statistic_msg('Pause')'''


if __name__ == '__main__':
    app = QApplication(sys.argv)
    ui = MainWindow()
    ui.show()
    sys.exit(app.exec_())
